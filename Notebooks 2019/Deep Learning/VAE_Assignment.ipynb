{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tyYDKlcqkBE",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment MSc Data Science**: a VAE for binarised MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B30ONxejRn-",
        "colab_type": "text"
      },
      "source": [
        "In this assignment, you'll play a bit with a VAE on a binarised version of MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3sguCcpg2Yw",
        "colab_type": "text"
      },
      "source": [
        "# Loading useful stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPwsirHNgywB",
        "colab_type": "code",
        "outputId": "2fec2419-3fc6-4c10-e107-130bc64d1e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMPYV_R2ghyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfd = tfp.distributions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h58v-MfihGen",
        "colab_type": "text"
      },
      "source": [
        "# Loading MNIST and binarising it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sTryqQpguSj",
        "colab_type": "code",
        "outputId": "6cf40040-af6d-4604-a6a0-b4b4b4501705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(train_images_nonbinary, y_train), (test_images_nonbinary,  y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images_nonbinary = train_images_nonbinary.reshape(train_images_nonbinary.shape[0], 28*28)\n",
        "test_images_nonbinary = test_images_nonbinary.reshape(test_images_nonbinary.shape[0], 28*28)\n",
        "\n",
        "y_train = tf.cast(y_train, tf.int64)\n",
        "y_test =tf.cast(y_test, tf.int64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP-RnOcwjIUK",
        "colab_type": "code",
        "outputId": "17338f50-3aad-45b7-b9e3-b2a539feab60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "plt.imshow(train_images_nonbinary[0, :].reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAGc0lEQVR4nO3dOWhVfx7G4bmjWChqSKMgiGihqEga\nFUQQkSCCFlGbgJViZcAqjZ1FRHApRItUgo1YujRaxKUQBHFpAvZKOo1L3Ii50w0M5H7zN8vkvcnz\nlHk5nlP44YA/Tmw0m81/AXn+Pd8PAExOnBBKnBBKnBBKnBBqaTU2Gg3/lAtzrNlsNib7uTcnhBIn\nhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBIn\nhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhFo63w/A\n/1qyZEm5r169ek7v39fX13Jbvnx5ee3mzZvL/cyZM+V++fLllltvb2957c+fP8v94sWL5X7+/Ply\nnw/enBBKnBBKnBBKnBBKnBBKnBBKnBDKOeck1q9fX+7Lli0r9z179pT73r17W24dHR3ltceOHSv3\n+fT+/ftyv3btWrn39PS03L5+/Vpe+/bt23J/+vRpuSfy5oRQ4oRQ4oRQ4oRQ4oRQ4oRQjWaz2Xps\nNFqPbayrq6vch4aGyn2uP9tKNTExUe4nT54s92/fvk373iMjI+X+6dOncn/37t207z3Xms1mY7Kf\ne3NCKHFCKHFCKHFCKHFCKHFCKHFCqEV5ztnZ2VnuL168KPeNGzfO5uPMqqmefXR0tNz379/fcvv9\n+3d57WI9/50p55zQZsQJocQJocQJocQJocQJocQJoRblr8b8+PFjuff395f74cOHy/3169flPtWv\niKy8efOm3Lu7u8t9bGys3Ldt29ZyO3v2bHkts8ubE0KJE0KJE0KJE0KJE0KJE0KJE0Ityu85Z2rV\nqlXlPtV/Vzc4ONhyO3XqVHntiRMnyv327dvlTh7fc0KbESeEEieEEieEEieEEieEEieEWpTfc87U\nly9fZnT958+fp33t6dOny/3OnTvlPtX/sUkOb04IJU4IJU4IJU4IJU4IJU4I5ZOxebBixYqW2/37\n98tr9+3bV+6HDh0q90ePHpU7/38+GYM2I04IJU4IJU4IJU4IJU4IJU4I5ZwzzKZNm8r91atX5T46\nOlrujx8/LveXL1+23G7cuFFeW/1dojXnnNBmxAmhxAmhxAmhxAmhxAmhxAmhnHO2mZ6ennK/efNm\nua9cuXLa9z537ly537p1q9xHRkamfe+FzDkntBlxQihxQihxQihxQihxQihxQijnnAvM9u3by/3q\n1avlfuDAgWnfe3BwsNwHBgbK/cOHD9O+dztzzgltRpwQSpwQSpwQSpwQSpwQSpwQyjnnItPR0VHu\nR44cablN9a1oozHpcd1/DQ0NlXt3d3e5L1TOOaHNiBNCiRNCiRNCiRNCiRNCOUrhH/v161e5L126\ntNzHx8fL/eDBgy23J0+elNe2M0cp0GbECaHECaHECaHECaHECaHECaHqgynazo4dO8r9+PHj5b5z\n586W21TnmFMZHh4u92fPns3oz19ovDkhlDghlDghlDghlDghlDghlDghlHPOMJs3by73vr6+cj96\n9Gi5r1279q+f6Z/68+dPuY+MjJT7xMTEbD5O2/PmhFDihFDihFDihFDihFDihFDihFDOOefAVGeJ\nvb29LbepzjE3bNgwnUeaFS9fviz3gYGBcr93795sPs6C580JocQJocQJocQJocQJocQJoRylTGLN\nmjXlvnXr1nK/fv16uW/ZsuWvn2m2vHjxotwvXbrUcrt79255rU++Zpc3J4QSJ4QSJ4QSJ4QSJ4QS\nJ4QSJ4RasOecnZ2dLbfBwcHy2q6urnLfuHHjtJ5pNjx//rzcr1y5Uu4PHz4s9x8/fvz1MzE3vDkh\nlDghlDghlDghlDghlDghlDghVOw55+7du8u9v7+/3Hft2tVyW7du3bSeabZ8//695Xbt2rXy2gsX\nLpT72NjYtJ6JPN6cEEqcEEqcEEqcEEqcEEqcEEqcECr2nLOnp2dG+0wMDw+X+4MHD8p9fHy83Ktv\nLkdHR8trWTy8OSGUOCGUOCGUOCGUOCGUOCGUOCFUo9lsth4bjdYjMCuazWZjsp97c0IocUIocUIo\ncUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUKo\n8ldjAvPHmxNCiRNCiRNCiRNCiRNCiRNC/QfM6zUP81ILVgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JaPwhRdhI8s",
        "colab_type": "text"
      },
      "source": [
        "Then we binarise the data. There are many ways to do that. Here, we simply round the numbers, following the [TF tutorial on convolutional VAEs](https://www.tensorflow.org/tutorials/generative/cvae)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyIpmbV8g4d3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizing the images to the range of [0., 1.]\n",
        "train_images = train_images_nonbinary/255.\n",
        "test_images = test_images_nonbinary/255.\n",
        "\n",
        "# Binarization\n",
        "train_images[train_images >= .5] = 1.\n",
        "train_images[train_images < .5] = 0.\n",
        "test_images[test_images >= .5] = 1.\n",
        "test_images[test_images < .5] = 0."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8pOg8LNg7p2",
        "colab_type": "code",
        "outputId": "be17a851-49ef-44b0-b45c-eaba2db377da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "plt.imshow(train_images[0, :].reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAADd0lEQVR4nO3dwU7bQBhG0UzF+7/ydNUNikKVwfEd\n+5wliEIXV7/EJ+Mx53wAPX/O/gGA58QJUeKEKHFClDgh6uvVJ8cYfpULB5tzjmcfdzkhSpwQJU6I\nEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKE\nKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVFfZ/8A7GPOufT1Y4xD\n//2V713kckKUOCFKnBAlTogSJ0SJE6LECVF2zhMcueeV3fX//S6XE6LECVHihChxQpQ4IUqcEGVK\necKv/N+z42NZZS4nRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBDlec4n\nVl9Vd9VX3XnO9bNcTogSJ0SJE6LECVHihChxQpQ4IcrO+YYz/z7rXb/3HbmcECVOiBInRIkTosQJ\nUeKEKHFClJ1zM6vPkrIPlxOixAlR4oQocUKUOCFKnBBlSjnBq7nDn5/kH5cTosQJUeKEKHFClDgh\nSpwQJU6IsnPGrL4+cHUn9chZh8sJUeKEKHFClDghSpwQJU6IEidE2Tk3s7qD/uTV19tAP8vlhChx\nQpQ4IUqcECVOiBInRIkTouycF3PkDupZ0c9yOSFKnBAlTogSJ0SJE6LECVHihCg7582sbI1HPiv6\neNhBv3M5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRHhm7mdXHvvgclxOi\nxAlR4oQocUKUOCFKnBAlToiyc27GTnkfLidEiROixAlR4oQocUKUOCFKnBBl54zZecf0Cr/f5XJC\nlDghSpwQJU6IEidEiROiTCkHMIfwG1xOiBInRIkTosQJUeKEKHFClDgh6pY758475E/slNfhckKU\nOCFKnBAlTogSJ0SJE6LECVHb7pxX3ipfsWPeh8sJUeKEKHFClDghSpwQJU6IEidEZXfOK++Ytkr+\nh8sJUeKEKHFClDghSpwQJU6IEidEZXdOWyB353JClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFK\nnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosaVX7UHO3M5IUqcECVOiBInRIkTosQJ\nUX8BqMZW74BKFnQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cybWUmUGMNqH",
        "colab_type": "text"
      },
      "source": [
        "# Deep latent variable model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_SffkK9GaRh",
        "colab_type": "text"
      },
      "source": [
        "We will use a **deep latent variable model with a Gaussian prior and a Bernoulli observation model**. This can be written:\n",
        "\n",
        "$$p_{\\boldsymbol{\\theta}}(\\mathbf{x}_1,...,\\mathbf{x}_n) = \\prod_{i=1}^n p(\\mathbf{x}_i|\\mathbf{z}_i)p(\\mathbf{z}_i),$$\n",
        "$$p(\\mathbf{z}_i) = \\mathcal{N}(\\mathbf{z}_i|\\mathbf{0}_d,\\mathbf{I}_d), $$\n",
        "$$p_{\\boldsymbol{\\theta}}(\\mathbf{x}_i|\\mathbf{z}_i) = \\mathcal{B} (\\mathbf{x}_i|\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i)),$$\n",
        "\n",
        "where $\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{[0,1]}^p$ is a function (called the **decoder**) parametrised by a deep neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdIQhO9ZMLsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = 50 # dimension of the latent space\n",
        "\n",
        "p_z = tfd.Independent(tfd.Normal(loc = tf.zeros(d, tf.float32), scale = tf.ones(d, tf.float32)),reinterpreted_batch_ndims=1)  # that's the prior"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1hb-6ysMLvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = 200 # number of hidden units (same for all MLPs)\n",
        "\n",
        "sigma = \"relu\"\n",
        "\n",
        "decoder = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[d,]),\n",
        "  tfkl.Dense(h, activation=sigma,kernel_initializer=\"orthogonal\"),\n",
        "  tfkl.Dense(h, activation=sigma,kernel_initializer=\"orthogonal\"),\n",
        "  tfkl.Dense(28*28) # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3HIWw5hNSun",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC62wDDvHIsb",
        "colab_type": "text"
      },
      "source": [
        "To be able to train our model, we will need an **encoder** (aka **inference network**), that will allow us to approximate the intractable posterior $p(\\mathbf{z}|\\mathbf{x})$.\n",
        "The approximate posterior is defined as follows\n",
        "$$ q(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x} ), \\text{Diag}(\\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x} )),\n",
        "$$ \n",
        "where $\\mathbf{x} \\mapsto (\\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x} ), \\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x} ))$ is a function from the data space (i.e. $\\{0,1\\}^p$) to $\\mathbb{R}^d \\times [0, \\infty[^d$ parametrised by a deep neural network. In other words, the encoder outputs the mean and the diagonal of the covariance matrix of the approximate posterior $q(\\mathbf{z}|\\mathbf{x})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFCFP4GMLxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[28*28,]),\n",
        "  tfkl.Dense(h, activation=sigma,kernel_initializer=\"orthogonal\"),\n",
        "  tfkl.Dense(h, activation=sigma,kernel_initializer=\"orthogonal\"),\n",
        "  tfkl.Dense(2*d) # 2*d because we need both the mean and the diagonal of the covariance\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbracCjgNDjH",
        "colab_type": "text"
      },
      "source": [
        "##The VAE objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2Nbj9JoNYIX",
        "colab_type": "text"
      },
      "source": [
        "The **VAE objective** is defined as\n",
        "$$\n",
        "\\mathcal{L}_1 (\\boldsymbol{\\theta,\\gamma}) = \\sum_{i=1}^n \\mathbb{E}_{\\mathbf{z} \\sim q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}_i)} \\left[ \\log\\frac{p_{\\boldsymbol{\\theta}}(\\mathbf{x}_i|\\mathbf{z})p(\\mathbf{z})}{q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}_i)} \\right].\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaHnLuxV1lf",
        "colab_type": "text"
      },
      "source": [
        "It is a lower bound of the likelihood of the deep latent variable model. Rather than the intractable likelihood, we'll **maximise this bound**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Wq-WzZOMx_",
        "colab_type": "text"
      },
      "source": [
        "To see more clearly the neural nets inside the objective, we may rewrite it as:\n",
        "$$\n",
        "\\mathcal{L}_1 (\\boldsymbol{\\theta,\\gamma}) = \\sum_{i=1}^n \\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{z} | \\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ), \\text{Diag}(\\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ))} \\left[ \\log\\frac{\\mathcal{B} (\\mathbf{x}_i|\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\mathcal{N}(\\mathbf{z}|\\mathbf{0}_d,\\mathbf{I}_d)}{\\mathcal{N}(\\mathbf{z} | \\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ), \\text{Diag}(\\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ))} \\right].\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRd-gC2qN5fS",
        "colab_type": "text"
      },
      "source": [
        "**Create a TF function that computes an unbiased estimate of it!** (using a single sample from the approximate posterior)\n",
        "\n",
        "A good way to go is to follow these steps, for all $\\mathbf{x}_i$ in your batch:\n",
        "\n",
        "\n",
        "*   Encode the batch fo data points, to get the $\\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ), \\text{Diag}(\\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ))$ \n",
        "*   Use these encoder outputs to define $q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}_i)$ as a TF probability distribution\n",
        "*   Take one sample $\\mathbf{z}_i$ from $q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}_i)$, and use it to:\n",
        "  *   Compute $\\mathcal{B} (\\mathbf{x}_i|\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i))$ by decoding $\\mathbf{z}_i$\n",
        "  *   Compute the rest of the log of $p(\\mathbf{z}_i)$ and $q_{\\boldsymbol{\\gamma}}(\\mathbf{z}_i|\\mathbf{x}_i)$ using the $\\texttt{.logprob}$ method from TF probability.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBAqrFf3ML30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def vae_bound(data):\n",
        "\n",
        "  # \n",
        "  \n",
        "  return(bound)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCqSPsmzPOs4",
        "colab_type": "text"
      },
      "source": [
        "Now optimise the objective!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnCA53ZiT8zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = list(encoder.trainable_variables) + list(decoder.trainable_variables)  \n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Xkdes82czr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(data):\n",
        "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\n",
        "    bound = vae_bound(data)\n",
        "    loss = - bound\n",
        "  gradients = tape.gradient(loss,params)  # here, the gradient is automatically computed\n",
        "  optimizer.apply_gradients(zip(gradients, params))  # Adam iteration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLC61FHpOt2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,y_train)).shuffle(60000).batch(batch_size) # TF creates the batches for us"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LfomSvOzKk-",
        "colab_type": "text"
      },
      "source": [
        "Write a training loop to optimise the VAE bound. Print the values of the training/test bounds after each epoch to check that it's working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PPgJy9pL9iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for images, _ in train_dataset:\n",
        "    ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyYAqDAfW-Le",
        "colab_type": "text"
      },
      "source": [
        "# Questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht3lxz73XDTX",
        "colab_type": "text"
      },
      "source": [
        "**1. Witnessing training**\n",
        "\n",
        "Plot the evolution of training and test likelihood bounds during training.\n",
        "\n",
        "Sample a bunch of digits (say, 20) from the model before, during, and after training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqStwTA1WnaU",
        "colab_type": "text"
      },
      "source": [
        "**2. Playing with intialisation strategies**\n",
        "\n",
        "What was the initialisation strategy used by default by Keras? Try different initialisation trategies that you learned in the course for the weights of the networks. Plot the training curves. Which ones do you think are better (in theory and in this particular example)?\n",
        "\n",
        "**Hint** You may simply use the $\\texttt{kernel_initialiser}$ argument:\n",
        "\n",
        "\n",
        "```\n",
        "  tfkl.Dense(h, activation=sigma, kernel_initializer='...'),\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld23kY_rXrAH",
        "colab_type": "text"
      },
      "source": [
        "**3. Representation learning**\n",
        "\n",
        "One of the applications of VAEs is to get a low-dimensional representation of the data. For example, one can replace the high-dimensional data points $\\mathbf{x}_i$ by the mean of the corresponding approximate posteriors $\\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ) \\in \\mathbb{R}^d$.\n",
        "\n",
        "Here, we trained a VAE with $d = 50$, which is not readily visualisable...\n",
        "\n",
        "Study ways to visualise the encodings $\\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i )$:\n",
        "\n",
        "*   Use PCA on the $50$-dimensional embedding $\\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i )$ to get a bidimensional embedding and visualise it!\n",
        "*   Train another VAE with $d = 2$, this will be readily visualisable!\n",
        "\n",
        "Which method do you think works best? Compare that to doing PCA with the original data.\n",
        "\n",
        "**Bonus** Come up with a way to quantify which method is better.\n",
        "\n",
        "**Bonus** Replace PCA by another dimension-reduction technique that you know.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6bhQr9SZ3ua",
        "colab_type": "text"
      },
      "source": [
        "**4. The reparametrisation trick**\n",
        "\n",
        "Thanks to TF probability, we did not have to implement the reparametrisation trick (cf last slides of Lecture 4). It was implicitly (and automatically) used in the line of code:\n",
        "\n",
        "```\n",
        "zgivenx = q_zgivenx.sample(1)\n",
        "```\n",
        "\n",
        "Replace this line of code by your own implementation of the trick (see slides of Lecture 4). Check that it works. Of course, you're not allowed to use $\\texttt{.sample}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4GiumIba3B7",
        "colab_type": "text"
      },
      "source": [
        "**5. The IWAE bound**\n",
        "\n",
        "Implement the tighter IWAE bound, and see if it gives better results.\n",
        "\n",
        "Whether or not you think that the IWAE is better or worse than the vanilla VAE, explain *why* you think so (faster/slower training? better/worse generative model? better/worse representation low-dimensional representation).\n",
        "\n",
        "**Hint** The beginning of the function to compute the bound is exaclty the same, until the line \n",
        "\n",
        "```\n",
        "zgivenx = q_zgivenx.sample(1)\n",
        "```\n",
        "\n",
        "that should be replaced by \n",
        "\n",
        "```\n",
        "zgivenx = q_zgivenx.sample(K)\n",
        "```\n",
        "because you want to sample $K>1$ importance weights.\n",
        "\n",
        "**Hint** The TF $\\texttt{reduce_logsumexp}$ the computes the [logsumexp function](https://en.wikipedia.org/wiki/LogSumExp) might come in handy.\n",
        "\n",
        "**Hint** For training, choose a small value of $K$ (say, between 5 and 50). It's much less computationnaly demanding.\n",
        "\n",
        "**Hint** For testing, choosing a larger value of $K$ will give you a better estimate of the test log-likelihood, allowing you to evaluate the quality of *any DLVM* (at least any VAE or IWAE) more accurately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIjBdPQpb6N3",
        "colab_type": "text"
      },
      "source": [
        "**6. (Bonus) The KL trick**\n",
        "\n",
        "Re-implement the VAE using the \"KL trick\" (cf Slide 14 of Lecture 4). If you already used the trick, implement a VAE *without* using it. Do you see a difference of convergence speed?\n",
        "\n",
        "**Hint** You may use the TF Porbability function $\\texttt{kl_divergence}$ or the exact formula for Gaussians (which will give the same results)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1SE_ryt_jG",
        "colab_type": "text"
      },
      "source": [
        "**7. (Bonus) Large scale IWAE bound**\n",
        "\n",
        "Compute the test IWAE bound with $K = 5000$. This metric is a reasonably tigh bound of the true test log likelihood, and is actually the most commonly used to evaluate DLVMs. Use it to compare several VAEs or IWAEs where you change a few building blocks.\n",
        "\n",
        "For example, you could compare several VAEs with different dimensions $d$, or compare an IWAE and a VAE, or try different activation functions, or network architectures..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s4-R37pZNjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}