{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BinaryMNIST_Bernoulli.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tyYDKlcqkBE",
        "colab_type": "text"
      },
      "source": [
        "# MSc Data Science: playing with binarised **MNIST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B30ONxejRn-",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, we'll play a bit with a binarised version of MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3sguCcpg2Yw",
        "colab_type": "text"
      },
      "source": [
        "# Loading useful stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPwsirHNgywB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMPYV_R2ghyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfd = tfp.distributions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h58v-MfihGen",
        "colab_type": "text"
      },
      "source": [
        "# Loading MNIST and binarising it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sTryqQpguSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_images_nonbinary, y_train), (test_images_nonbinary,  y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images_nonbinary = train_images_nonbinary.reshape(train_images_nonbinary.shape[0], 28*28)\n",
        "test_images_nonbinary = test_images_nonbinary.reshape(test_images_nonbinary.shape[0], 28*28)\n",
        "\n",
        "y_train = tf.cast(y_train, tf.int64)\n",
        "y_test =tf.cast(y_test, tf.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP-RnOcwjIUK",
        "colab_type": "code",
        "outputId": "d0d6894f-9769-4497-c200-cda6a00bace3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "plt.imshow(train_images_nonbinary[0, :].reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JaPwhRdhI8s",
        "colab_type": "text"
      },
      "source": [
        "Then we binarise the data. There are many ways to do that. Here, we simply round the numbers, following the [TF tutorial on convolutional VAEs](https://www.tensorflow.org/tutorials/generative/cvae)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyIpmbV8g4d3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizing the images to the range of [0., 1.]\n",
        "train_images = train_images_nonbinary/255.\n",
        "test_images = test_images_nonbinary/255.\n",
        "\n",
        "# Binarization\n",
        "train_images[train_images >= .5] = 1.\n",
        "train_images[train_images < .5] = 0.\n",
        "test_images[test_images >= .5] = 1.\n",
        "test_images[test_images < .5] = 0."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8pOg8LNg7p2",
        "colab_type": "code",
        "outputId": "ade408cc-cc69-4622-bc73-38d4fa4d2705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "plt.imshow(train_images[0, :].reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADd0lEQVR4nO3dwU7bQBhG0UzF+7/ydNUNikKVwfEd\n+5wliEIXV7/EJ+Mx53wAPX/O/gGA58QJUeKEKHFClDgh6uvVJ8cYfpULB5tzjmcfdzkhSpwQJU6I\nEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKE\nKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVFfZ/8A7GPOufT1Y4xD\n//2V713kckKUOCFKnBAlTogSJ0SJE6LECVF2zhMcueeV3fX//S6XE6LECVHihChxQpQ4IUqcEGVK\necKv/N+z42NZZS4nRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBDlec4n\nVl9Vd9VX3XnO9bNcTogSJ0SJE6LECVHihChxQpQ4IcrO+YYz/z7rXb/3HbmcECVOiBInRIkTosQJ\nUeKEKHFClJ1zM6vPkrIPlxOixAlR4oQocUKUOCFKnBBlSjnBq7nDn5/kH5cTosQJUeKEKHFClDgh\nSpwQJU6IsnPGrL4+cHUn9chZh8sJUeKEKHFClDghSpwQJU6IEidE2Tk3s7qD/uTV19tAP8vlhChx\nQpQ4IUqcECVOiBInRIkTouycF3PkDupZ0c9yOSFKnBAlTogSJ0SJE6LECVHihCg7582sbI1HPiv6\neNhBv3M5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRHhm7mdXHvvgclxOi\nxAlR4oQocUKUOCFKnBAlToiyc27GTnkfLidEiROixAlR4oQocUKUOCFKnBBl54zZecf0Cr/f5XJC\nlDghSpwQJU6IEidEiROiTCkHMIfwG1xOiBInRIkTosQJUeKEKHFClDgh6pY758475E/slNfhckKU\nOCFKnBAlTogSJ0SJE6LECVHb7pxX3ipfsWPeh8sJUeKEKHFClDghSpwQJU6IEidEZXfOK++Ytkr+\nh8sJUeKEKHFClDghSpwQJU6IEidEZXdOWyB353JClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFK\nnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosaVX7UHO3M5IUqcECVOiBInRIkTosQJ\nUX8BqMZW74BKFnQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukx-euwihWT0",
        "colab_type": "text"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqCIj_dXjv-o",
        "colab_type": "text"
      },
      "source": [
        "Let us first build a small neural net to classify MNIST. The model is:\n",
        "$$ p (y | \\mathbf{x} ) = \\text{Cat} (y |\\text{Softmax}(\\mathbf{W}_1\\sigma(\\mathbf{W}_0\\mathbf{x}+\\mathbf{b}_0)+\\mathbf{b}_1)).$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO7T-0Uijvbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = 200 # number of hidden units\n",
        "sigma = \"relu\" # activation function\n",
        "\n",
        "classifier = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[28*28,]),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(10), # because we have 10 classes\n",
        "  tfkl.Softmax()\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3js5oRp4xLTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZl_rr_thoU9",
        "colab_type": "text"
      },
      "source": [
        "To train the classifier, we define a function that performs a gradient descent step. First, we choose the flavour of SGD that we want (in this case, Adam)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fDvvHAEq2BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOtN8V7Dh56N",
        "colab_type": "text"
      },
      "source": [
        "We can now define the gradient step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WwxktDPq2Do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(data, labels):\n",
        "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\n",
        "    preds = classifier(data)\n",
        "    p_ygivenx_model = tfd.Categorical(probs = preds) # One could also use logits rather than probs and remove the softmax layer...\n",
        "    logp_ygivenx = p_ygivenx_model.log_prob(labels)\n",
        "    loss = -tf.reduce_mean(logp_ygivenx)  # the loss is the average negative log likelihood\n",
        "  gradients = tape.gradient(loss, classifier.trainable_variables)  # here, the gradient is automatically computed\n",
        "  optimizer.apply_gradients(zip(gradients, classifier.trainable_variables))  # Adam iteration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUTLqnFfilpS",
        "colab_type": "text"
      },
      "source": [
        "To evaluate the quality of our model, we will write a function that computes both accuracy and log_likelihood. We don't care about gradients here bacause it's only for evaluation (not for training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3SdSZiDCa7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def evaluate(data, labels):\n",
        "  preds = classifier(data)\n",
        "  p_ygivenx_model = tfd.Categorical(probs = preds)\n",
        "  logp_ygivenx = p_ygivenx_model.log_prob(labels)\n",
        "  log_likelihood = tf.reduce_mean(logp_ygivenx)\n",
        "  y_pred = tf.argmax(preds,1)\n",
        "  acc = tf.reduce_mean(tf.cast(y_pred == labels,tf.float32))\n",
        "  return acc, log_likelihood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Xkdes82czr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,y_train)).shuffle(60000).batch(32) # TF creates the batches for us"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRcuargKLj9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po_n-Kb6q2F4",
        "colab_type": "code",
        "outputId": "6d543ec7-dd64-4c6c-d224-4e12713b96fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for images, labels in train_dataset:\n",
        "    train_step(images, labels) # Adam iteration\n",
        "  acc, log_likelihood = evaluate(train_images,y_train)\n",
        "  acc_test, log_likelihood_test = evaluate(test_images,y_test)\n",
        "  print('Epoch  %g' %epoch)\n",
        "  print('Train accuracy  %g' %acc.numpy())\n",
        "  print('Test accuracy  %g' %acc_test.numpy())\n",
        "  print('Train log-likelihood  %g' %log_likelihood.numpy())\n",
        "  print('Test log-likelihood  %g' %log_likelihood_test.numpy())\n",
        "  print('-----------')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer dense_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch  1\n",
            "Train accuracy  0.911933\n",
            "Test accuracy  0.9113\n",
            "Train log-likelihood  -0.31469\n",
            "Test log-likelihood  -0.308632\n",
            "-----------\n",
            "Epoch  2\n",
            "Train accuracy  0.932317\n",
            "Test accuracy  0.9309\n",
            "Train log-likelihood  -0.242203\n",
            "Test log-likelihood  -0.244548\n",
            "-----------\n",
            "Epoch  3\n",
            "Train accuracy  0.94525\n",
            "Test accuracy  0.9408\n",
            "Train log-likelihood  -0.196561\n",
            "Test log-likelihood  -0.205875\n",
            "-----------\n",
            "Epoch  4\n",
            "Train accuracy  0.954\n",
            "Test accuracy  0.9489\n",
            "Train log-likelihood  -0.167545\n",
            "Test log-likelihood  -0.17856\n",
            "-----------\n",
            "Epoch  5\n",
            "Train accuracy  0.960983\n",
            "Test accuracy  0.9552\n",
            "Train log-likelihood  -0.143051\n",
            "Test log-likelihood  -0.156956\n",
            "-----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1pPoLI6jlKI",
        "colab_type": "text"
      },
      "source": [
        "# Simple generative modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKb773pujpvx",
        "colab_type": "text"
      },
      "source": [
        "We build here a **super simple** generative model for the data set. Specifically, the model is simply a product of Bernoulli distributions:\n",
        "$$p (\\textbf{x}) = \\prod_{j = 1}^p \\mathcal{B}(x_j|\\pi_j).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5caP3FP1kmaF",
        "colab_type": "text"
      },
      "source": [
        "Why is this clearly a pretty bad model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTLh2xtFkqzA",
        "colab_type": "text"
      },
      "source": [
        "Training can simply be done by computing the means of all the pixels independently:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUtIAt3nlhgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pis_bern = train_images.mean(0) + 10**(-10) # I just add 10^-10 to avoid zeros and numerical problems "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKeaKSYIltiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prod_of_bernoullis =  tfd.Independent(distribution  = tfd.Bernoulli(probs = pis_bern), reinterpreted_batch_ndims = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWZLQLwbhk9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prod_of_bernoullis = tfd.Independent(distribution  = tfd.Bernoulli(probs = pis_bern), reinterpreted_batch_ndims=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJILKmwriDNG",
        "colab_type": "code",
        "outputId": "7e68c84c-5aea-4628-9ee4-13b7441a763b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "sample_pofbern = tf.reshape(prod_of_bernoullis.sample(1),(28,28))\n",
        "plt.imshow(sample_pofbern, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEL0lEQVR4nO3dwWrcMABF0br0/3/ZXXU32B0UoSv7\nnGVLk5kpF0Eeio/zPH8BPb9XvwDgM3FClDghSpwQJU6I+nP1l8dx+FEuTHae5/Hpz52cECVOiBIn\nRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKjL\nX43Jfu4eTHUcH38LY95T39cVJydEiROixAlR4oQocUKUOCFKnBBl51zgarObvdfN3Atnfu0n7ph3\nnJwQJU6IEidEiROixAlR4oQocUKUnXOBmZvd3de+2yJnfm++4+SEKHFClDghSpwQJU6IEidEiROi\n7JwLjGyNozumLXIfTk6IEidEiROixAlR4oQocUKUKWWBqznjbgqZeeVrNjPPd5ycECVOiBInRIkT\nosQJUeKEKHFClJ0z5smPAOQ7Tk6IEidEiROixAlR4oQocUKUOCFq251z5V638tdTjn5tdyb34eSE\nKHFClDghSpwQJU6IEidEiROijqvd7DiOR17QW/37U28+86nfe6WZjz7c2XmeH9+ckxOixAlR4oQo\ncUKUOCFKnBAlToja9j7nzsqb3cotcuU91yInJ0SJE6LECVHihChxQpQ4IeqVU8rqH6uPXBmbPRlc\n/fvRR/zN/NxX/5/O4OSEKHFClDghSpwQJU6IEidEiROiXrlzrjayJc7eQcsb7Ns4OSFKnBAlTogS\nJ0SJE6LECVHihCg75wQz977RO5V3Zt7ntIN+x8kJUeKEKHFClDghSpwQJU6IEidEvXLn3HlvG71T\nufIRf3dG7pI+kZMTosQJUeKEKHFClDghSpwQJU6IeuXOufNmNnujHdlBZ981HbHjtu3khChxQpQ4\nIUqcECVOiBInRB0313SyPxvf8UfjP2F0rpj5uZRfW9l5nh/fuJMTosQJUeKEKHFClDghSpwQJU6I\n2vbK2MzH6K3c21ZfCZv5CMAnf64zODkhSpwQJU6IEidEiROixAlR4oSobXfOspmb2ujXHvneKzfW\nUcUd846TE6LECVHihChxQpQ4IUqcECVOiHrlzrnj5vW/yo/he/LnPoOTE6LECVHihChxQpQ4IUqc\nEPXKKWW2lVefRq9l3TwS8vLf8rOcnBAlTogSJ0SJE6LECVHihChxQtS2O+eOj3T7Z+Ra1+z3Xf7c\n3sbJCVHihChxQpQ4IUqcECVOiBInRG27c87c48pb4t1rc5/zOZycECVOiBInRIkTosQJUeKEKHFC\n1LY7J3PYMjucnBAlTogSJ0SJE6LECVHihChxQpSd84Py1ld+bfwsJydEiROixAlR4oQocUKUOCFK\nnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDgh6jjPc/VrAD5wckKU\nOCFKnBAlTogSJ0SJE6L+Agsv1CEBxpckAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyDKnZgGpbVZ",
        "colab_type": "text"
      },
      "source": [
        "We usually assess the quality of such generative models by computing the **test log-likelihood**. Here, we can do that easily because the model is simple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLYwDGrUl77G",
        "colab_type": "code",
        "outputId": "8ec6e9c1-3be4-4423-ad9b-86733c19be74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reduce_mean(prod_of_bernoullis.log_prob(test_images)).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-205.84973740976156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnj_Px1kpu8n",
        "colab_type": "text"
      },
      "source": [
        "The state of the art for binarised versions of MNIST is around -75, which is much higher than that! Using a VAE (or an IWAE), try to close this gap!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS2lz866ECey",
        "colab_type": "text"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTPgkswQEdhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = 10 # dimension of latent space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PPgJy9pL9iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[d,]),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(28*28),  # I don't care (YET) about the fact that those are probabilities (in [0,1])\n",
        "])\n",
        "\n",
        "encoder = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[28*28,]),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(2*d), # I don't care (YET) about the fact that half of these must be non-negative (diagonal of covariance)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
