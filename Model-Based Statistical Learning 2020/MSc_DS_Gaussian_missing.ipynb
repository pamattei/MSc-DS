{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSc_DS_Gaussian_missing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-tmkEW3V8IS"
      },
      "source": [
        "# MSc DS: Learning a Gaussian with missing values via SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plzaLT0FkQth"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.io\n",
        "import scipy.sparse\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTAODH-sWDIe"
      },
      "source": [
        "We load the Iris data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOGz0cVhkcit"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris(True)[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqsBscE6kymT"
      },
      "source": [
        "We now standardise the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYd-DUg8k1Eq"
      },
      "source": [
        "xfull = ((data - np.mean(data,0))/np.std(data,0)).astype(np.float32)\n",
        "n = xfull.shape[0] # number of observations\n",
        "p = xfull.shape[1] # number of feat*ures"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4TLwBEWk3Bh"
      },
      "source": [
        "We will remove uniformy at random 10% of the data. This corresponds to a *missing completely at random (MCAR)* scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS23b7FIknAk"
      },
      "source": [
        "perc_miss = 0.1 # 50% of missing data\n",
        "xmiss = np.copy(xfull)\n",
        "xmiss_flat = xmiss.flatten()\n",
        "miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int), replace=False)\n",
        "xmiss_flat[miss_pattern] = np.nan \n",
        "xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
        "mask = np.isfinite(xmiss) # binary mask that indicates which values are missing"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3G4H2Kpp11T"
      },
      "source": [
        ""
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dan0dolalEN9"
      },
      "source": [
        "We want to learn a Gaussian distribution:\n",
        "$$p(x) = \\mathcal{N}(x|\\mu,\\Sigma), $$\n",
        "where $\\Sigma$ is a diagonal matrix, using maximum likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_H06w5lABX"
      },
      "source": [
        "mu = tf.Variable(tf.ones([p]), dtype=tf.float32)\n",
        "log_sigma_diag = tf.Variable(tf.zeros([p]), dtype=tf.float32) # log-sd of the Gaussian"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVn7rlT3WUfE"
      },
      "source": [
        "We first use define a function that can compute the likelihood of a complete data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdjN86w7lNWU"
      },
      "source": [
        "@tf.function\n",
        "def log_likelihood(x):\n",
        "  sigma_diag = tf.exp(log_sigma_diag)\n",
        "  p_x = tfd.MultivariateNormalDiag(loc = mu, scale_diag = sigma_diag)\n",
        "  return(p_x.log_prob(x))"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vZkpyQTV7he"
      },
      "source": [
        "Then, a similar one than can compute $\\log p(x^{obs})$, which is the relevant quantity to look at under MCAR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4u8DM8Wu_40"
      },
      "source": [
        "@tf.function\n",
        "def log_likelihood_incomplete(x,m): # log(p(x_obs))\n",
        "  x = tf.squeeze(x)\n",
        "  m = tf.squeeze(m)\n",
        "  mean = mu[m]\n",
        "  sigma_diag = tf.exp(log_sigma_diag[m])\n",
        "  p_x_obs = tfd.MultivariateNormalDiag(loc = mean, scale_diag = sigma_diag)\n",
        "  return(p_x_obs.log_prob(x[m]))"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1NmGfXvWlaj"
      },
      "source": [
        "Now we perform SGD, first on complete data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB13ZT7mmjdE"
      },
      "source": [
        "params = [mu] + [log_sigma_diag]\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRNNN_vglNYo"
      },
      "source": [
        "def train_step(data):\n",
        "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\n",
        "    loss = -log_likelihood(data)  # the loss is the average negative log likelihood\n",
        "  gradients = tape.gradient(loss, params)  # here, the gradient is automatically computed\n",
        "  optimizer.apply_gradients(zip(gradients, params))  # Adam iteration"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PpFGtqOvfpp"
      },
      "source": [
        "def train_step_incomplete(data,mask):\n",
        "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\n",
        "    loss = -log_likelihood_incomplete(data,mask)  # the loss is the average negative log likelihood\n",
        "  gradients = tape.gradient(loss, params)  # here, the gradient is automatically computed\n",
        "  optimizer.apply_gradients(zip(gradients, params))  # Adam iteration"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hx_qtRhmZsP"
      },
      "source": [
        "train_data_complete = tf.data.Dataset.from_tensor_slices(xfull).shuffle(p).batch(1) "
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS_OzM3PlMd_",
        "outputId": "b02157c7-3bab-4116-f027-74cdc19741c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for data in train_data_complete:\n",
        "    train_step(data) # Adam iteration\n",
        "  if (epoch % 100) == 1:\n",
        "    ll_train = tf.reduce_mean(log_likelihood(xfull))\n",
        "    print('Epoch  %g' %epoch)\n",
        "    print('Training log-likelihood %g' %ll_train.numpy())\n",
        "    print('Mean %g')\n",
        "    tf.print(mu)\n",
        "    print('-----------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1\n",
            "Training log-likelihood -7.62358\n",
            "Mean %g\n",
            "[0.991388202 0.989105403 0.992225766 0.992002726]\n",
            "-----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9azLw1pWrZK"
      },
      "source": [
        "And now on incomplete data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcP3gQHbmyd2"
      },
      "source": [
        "train_data_incomplete = tf.data.Dataset.from_tensor_slices((xmiss,mask)).shuffle(p).batch(1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHxla7VDnez3"
      },
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for data,m in train_data_incomplete:\n",
        "    train_step_incomplete(data,m) # Adam iteration\n",
        "  if (epoch % 100) == 1:\n",
        "    ll_train = tf.reduce_mean(log_likelihood(xfull))\n",
        "    print('Epoch  %g' %epoch)\n",
        "    print('Training log-likelihood %g' %ll_train.numpy())\n",
        "    print('Mean %g')\n",
        "    tf.print(mu)\n",
        "    print('-----------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLsv0I8_vr1D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}