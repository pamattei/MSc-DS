{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSc_DS_Gaussian_fullcov.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-tmkEW3V8IS"
      },
      "source": [
        "# MSc DS: Dealing with full convariance Gaussians with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plzaLT0FkQth"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.io\n",
        "import scipy.sparse\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTAODH-sWDIe"
      },
      "source": [
        "We load the Iris data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOGz0cVhkcit"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris(True)[0]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqsBscE6kymT"
      },
      "source": [
        "We now standardise the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYd-DUg8k1Eq"
      },
      "source": [
        "xfull = ((data - np.mean(data,0))/np.std(data,0)).astype(np.float32)\n",
        "n = xfull.shape[0] # number of observations\n",
        "p = xfull.shape[1] # number of feat*ures"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dan0dolalEN9"
      },
      "source": [
        "We want to learn a Gaussian distribution:\n",
        "$$p(x) = \\mathcal{N}(x|\\mu,\\Sigma), $$\n",
        "where $\\Sigma$ is a not a diagonal matrix, using maximum likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFVw_oQIEvuZ"
      },
      "source": [
        "We want to use stochastic gradient techniques to learn $\\mu$ and $\\Sigma$. The issue is that SGD works best on unconstrained Euclidean spaces like $\\mathbb{R}^K$, and $\\Sigma$ lives in a constrained space (the space of positive definite matrices). We need to **reparametrise the model with unconstrained parameters.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czm_sY4CFbHo"
      },
      "source": [
        "A solution is provided by something called the **[Cholesky decomposition.](https://en.wikipedia.org/wiki/Cholesky_decomposition)** Any positive-definite matrix $\\Sigma$ can be uniquely written as a product\n",
        "$$\\Sigma = L L^T,$$\n",
        "where $L$ is a **lower-triangular matrix with strictly positive diagonal entries**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5o4XjFIFbKJ"
      },
      "source": [
        "The issue is that there is still a constraint on $L$. Namely, the diagonal has to be strictly positive. A simple way to enforce that is to define another matrix $C$ such that\n",
        "$$\\forall i \\neq j, \\; c_{ij} = l_{ij}$$\n",
        "$$\\forall i , \\; c_{ii} = \\log(l_{ii}),$$\n",
        "which is just applying a log to the diagonal of $L$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KedlaO6mJXnO"
      },
      "source": [
        "Since the log function sends $]0,\\infty[$ to the whole real line, **$C$ will be an unconstrained matrix lower-triangular matrix.** So, we will use $C$ as a parameter to model the covariance, rather than $\\Sigma$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cODDrYKZMcDP"
      },
      "source": [
        "Triangular matrices in TF can be created this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnAcaliVMbV8",
        "outputId": "2ee52ab2-0ccf-4122-9726-c11287005144"
      },
      "source": [
        "dim_triangular_matrix = int(p*(p+1)/2)\n",
        "tfp.math.fill_triangular(tf.random.normal([dim_triangular_matrix]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
              "array([[-0.99635327,  0.        ,  0.        ,  0.        ],\n",
              "       [-0.7582339 , -0.6961709 ,  0.        ,  0.        ],\n",
              "       [ 0.15100579, -1.0348179 ,  0.94574684,  0.        ],\n",
              "       [-1.1293083 ,  1.5236211 , -0.64511794, -0.24113938]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo4gAMwwMwFq"
      },
      "source": [
        "We can use that to finally create our variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6eGQxjSMyuH"
      },
      "source": [
        "mu = tf.Variable(tf.random.normal([p]), dtype=tf.float32)\n",
        "C = tf.Variable(tfp.math.fill_triangular(tf.random.normal([dim_triangular_matrix])), dtype=tf.float32) # log-sd of the Gaussian"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwtrAXjAM7h-"
      },
      "source": [
        "Now, how do we compute $\\Sigma$ using $C$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDiOxj3eL5kg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTSNZOysNmDd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_H06w5lABX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVn7rlT3WUfE"
      },
      "source": [
        "We first use define a function that can compute the likelihood of a complete data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdjN86w7lNWU"
      },
      "source": [
        "@tf.function\n",
        "def log_likelihood(x):\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1NmGfXvWlaj"
      },
      "source": [
        "Now we perform SGD!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB13ZT7mmjdE"
      },
      "source": [
        "params = [mu] + [C]\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRNNN_vglNYo"
      },
      "source": [
        "def train_step(data):\n",
        "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\n",
        "    loss = -log_likelihood(data)  # the loss is the average negative log likelihood\n",
        "  gradients = tape.gradient(loss, params)  # here, the gradient is automatically computed\n",
        "  optimizer.apply_gradients(zip(gradients, params))  # Adam iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hx_qtRhmZsP"
      },
      "source": [
        "train_data_complete = tf.data.Dataset.from_tensor_slices(xfull).shuffle(p).batch(1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS_OzM3PlMd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2781454c-1938-40b8-8f55-baad9249e6e3"
      },
      "source": [
        "EPOCHS = 601\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for data in train_data_complete:\n",
        "    train_step(data) # Adam iteration\n",
        "  if (epoch % 100) == 1:\n",
        "    ll_train = tf.reduce_mean(log_likelihood(xfull))\n",
        "    print('Epoch  %g' %epoch)\n",
        "    print('Training log-likelihood %g' %ll_train.numpy())\n",
        "    print('-----------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1\n",
            "Training log-likelihood -11.4537\n",
            "Mean\n",
            "-----------\n",
            "Epoch  101\n",
            "Training log-likelihood -3.29557\n",
            "Mean\n",
            "-----------\n",
            "Epoch  201\n",
            "Training log-likelihood -3.29002\n",
            "Mean\n",
            "-----------\n",
            "Epoch  301\n",
            "Training log-likelihood -3.28909\n",
            "Mean\n",
            "-----------\n",
            "Epoch  401\n",
            "Training log-likelihood -3.29002\n",
            "Mean\n",
            "-----------\n",
            "Epoch  501\n",
            "Training log-likelihood -3.28925\n",
            "Mean\n",
            "-----------\n",
            "Epoch  601\n",
            "Training log-likelihood -3.29218\n",
            "Mean\n",
            "-----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9azLw1pWrZK"
      },
      "source": [
        "And now on incomplete data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLsv0I8_vr1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8309246d-ebbf-4889-ff63-4fc505b54c68"
      },
      "source": [
        "  L = C\n",
        "  L = tf.linalg.set_diag(L,tf.exp(tf.linalg.diag_part(C)))\n",
        "  Sigma = tf.matmul(L,L, transpose_b=True)\n",
        "  print(Sigma)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 1.0228177  -0.11411762  0.8826632   0.8174147 ]\n",
            " [-0.11411762  0.9998517  -0.423512   -0.3605057 ]\n",
            " [ 0.8826632  -0.423512    1.0056474   0.9578774 ]\n",
            " [ 0.8174147  -0.3605057   0.9578774   1.0072082 ]], shape=(4, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39h2XpL3P1Yg",
        "outputId": "c228bcd7-8a68-4bd1-a4ee-65935f2c5d90"
      },
      "source": [
        " np.cov(xfull, rowvar=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.00671141, -0.11835884,  0.87760447,  0.82343068],\n",
              "       [-0.11835884,  1.0067114 , -0.43131554, -0.36858316],\n",
              "       [ 0.87760447, -0.43131554,  1.0067114 ,  0.96932763],\n",
              "       [ 0.82343068, -0.36858316,  0.96932763,  1.00671144]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SUDnXGbQDO3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo6V9bF7VQtE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}