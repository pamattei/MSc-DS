{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_MSc_Data_Science_Deep_Discriminative_Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jarvIzm6TMq_"
      },
      "source": [
        "# MSc Data Science: (deep) discriminative models for **MNIST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3sguCcpg2Yw"
      },
      "source": [
        "# Loading useful stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMPYV_R2ghyx"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfd = tfp.distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zTFbzrTaie"
      },
      "source": [
        "#Loading and normalising MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo-tSpBxTMVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1c57b6-0b92-4929-a168-bb5af20a61c0"
      },
      "source": [
        "(train_images, y_train), (test_images,  y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28*28)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28*28)\n",
        "\n",
        "y_train = tf.cast(y_train, tf.int32)\n",
        "y_test =tf.cast(y_test, tf.int32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "dokv8mqDTMYD",
        "outputId": "dc9c5f38-5f1d-438f-a10a-979a1aea23e7"
      },
      "source": [
        "plt.imshow(train_images[0, :].reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oshsOP18ToYt"
      },
      "source": [
        "# Normalizing the images to the range of [0., 1.]\n",
        "train_images = train_images/255.\n",
        "test_images = test_images/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m32sDnj6Txzb"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJwgLwUnT23s"
      },
      "source": [
        "Our goal is to build a classifier on MNIST. A first simple example of classifier is **logistic regression**, a particular case of **discriminative model**. The model for (multiclass) logistic regression is \n",
        "$$ p (y | \\mathbf{x} ) = \\text{Cat} (y |\\text{Softmax}(\\mathbf{W}\\mathbf{x}+\\mathbf{b})),$$\n",
        "where the unknown parameters are $\\mathbf{W}$ and $\\mathbf{b}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-2YQtR9Uiqk"
      },
      "source": [
        "**Question 1.** What are the dimensions of $\\mathbf{W}$ and $\\mathbf{b}$? What is the total number of parameters in the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1UhiLHDU4da"
      },
      "source": [
        "We will build our logistic regression model using [**keras**](https://keras.io/), a nice deep learning API. In particular, keras's [sequential model](https://keras.io/guides/sequential_model/) is simple way of building compositions of parametric functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpTTmZ36T2T9"
      },
      "source": [
        "logistic_regression = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[28*28,]),\n",
        "  tfkl.Dense(10, kernel_initializer=initializers.RandomNormal(stddev=1)) # because we have 10 classes\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj4QfIp0vKff"
      },
      "source": [
        "Here, $\\texttt{logistic_regression}$ represents the function $ \\mathbf{x} \\mapsto \\mathbf{W}\\mathbf{x}+\\mathbf{b}$, that takes vectors as inputs, and returns probabilities for each class. We can try with the first MNIST image. The model is initialised by sampling each coefficient of $\\mathbf{W}$ from a standard Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY-1z5qbUuKc",
        "outputId": "e27aab9a-1f2d-4002-8ba6-af2236b1b631"
      },
      "source": [
        "logistic_regression(train_images[0:1,])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[  8.495928  ,   5.488267  ,  20.32993   ,  -3.6566849 ,\n",
              "         -3.0612555 ,   5.8395925 ,  -3.9733167 ,   1.7556076 ,\n",
              "         -0.06998014, -11.785327  ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijfw4WfpvsP_"
      },
      "source": [
        "Note that the output is a Tensorflow tensor. One can easily get a Numpy array instead this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHqYNRStUuMz",
        "outputId": "3074e8cd-01d6-4e23-ed7e-856211d4a01e"
      },
      "source": [
        "logistic_regression(train_images[0:1,]).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  8.495928  ,   5.488267  ,  20.32993   ,  -3.6566849 ,\n",
              "         -3.0612555 ,   5.8395925 ,  -3.9733167 ,   1.7556076 ,\n",
              "         -0.06998014, -11.785327  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umvYlfGBv2TX"
      },
      "source": [
        "This $\\texttt{logistic_regression}$ conveniently can also handle **batches** of inputs. Here we look at the predictions of the 10 first digits of MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OC4ZgXVv1hc",
        "outputId": "82bbfcd8-fe00-4e51-9b93-3c51818f76a4"
      },
      "source": [
        "logistic_regression(train_images[0:10,]).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8.4959307e+00,  5.4882679e+00,  2.0329935e+01, -3.6566839e+00,\n",
              "        -3.0612559e+00,  5.8395915e+00, -3.9733174e+00,  1.7556089e+00,\n",
              "        -6.9981225e-02, -1.1785323e+01],\n",
              "       [ 1.2374611e+00,  5.0504885e+00,  1.3919871e+01,  1.8583012e+01,\n",
              "        -7.7279681e-01,  7.7924519e+00,  3.0808744e+00,  6.1116562e+00,\n",
              "        -9.3280048e+00,  4.9655461e+00],\n",
              "       [-7.2871075e+00, -1.5397094e+01, -1.0909403e+01,  7.9048395e+00,\n",
              "         6.6938150e-01, -1.1753743e+01, -1.0543722e+01,  4.1888528e+00,\n",
              "        -1.2670908e+01, -9.9815159e+00],\n",
              "       [ 9.8974733e+00,  5.1582203e+00,  1.2600046e+01,  7.4502740e+00,\n",
              "        -1.0253202e+01, -5.3853106e+00, -1.7099901e+01, -7.8233685e+00,\n",
              "        -2.9905379e+00,  7.6379871e+00],\n",
              "       [-6.8080902e+00, -8.9221420e+00, -1.5757835e+00,  3.5098949e-03,\n",
              "        -1.2371784e+00, -1.3925304e+01, -5.5616784e+00, -6.6310763e-01,\n",
              "        -1.0669029e+01, -1.0636754e+00],\n",
              "       [ 4.1798277e+00,  1.1117022e+00,  1.6029219e+01,  6.1082716e+00,\n",
              "        -7.9128904e+00, -1.2937068e+01, -7.3281417e+00, -2.4463954e+00,\n",
              "         3.0534830e+00,  3.4753108e+00],\n",
              "       [ 8.8484545e+00,  6.3316864e-01,  1.5874753e+00, -3.4516423e+00,\n",
              "        -1.5567224e+01, -9.9952574e+00, -8.9792271e+00, -1.4690780e+01,\n",
              "        -6.3155788e-01, -1.1914693e+01],\n",
              "       [-8.7821283e+00,  2.9177175e+00,  6.4880133e+00,  7.8736238e+00,\n",
              "        -3.1192892e+00, -2.6221457e+00, -6.6073079e+00,  3.6180174e+00,\n",
              "        -6.1136546e+00,  1.2007644e+00],\n",
              "       [ 7.0237122e+00, -1.7929100e-01, -5.9667659e-01, -3.3677773e+00,\n",
              "        -6.6392465e+00, -1.4650140e+01, -2.5470419e+00, -8.1242037e+00,\n",
              "        -8.7048221e-01,  1.6601335e-01],\n",
              "       [ 7.9572091e+00, -5.9142938e+00,  3.9319346e+00,  6.7513628e+00,\n",
              "        -2.2728748e+00, -6.7125788e+00,  2.5103524e+00, -7.0723972e+00,\n",
              "        -1.2723814e+01,  1.8114733e+00]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v65KRekywEtx"
      },
      "source": [
        "One can check that each row of these predictions sums to one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9dvwvD5UuPQ",
        "outputId": "81d6e570-fb35-4b9f-cc1d-4daf0ca45b28"
      },
      "source": [
        "np.sum(logistic_regression(train_images[0:10,]).numpy(),1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 19.362774 ,  50.640556 , -65.78042  ,  -0.8083191, -50.422478 ,\n",
              "         3.3333173, -54.16128  ,  -5.146389 , -29.785133 , -11.733627 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LEUC7LCz_wG"
      },
      "source": [
        "One can us Tensorflow Probability to create the distribution  $p (y | \\mathbf{x} )$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tGJ3oYdT2WE"
      },
      "source": [
        "p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(train_images[0:10,]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHLCouzqwQBW",
        "outputId": "80d10882-b945-47b9-f1cf-4c316ad7a3aa"
      },
      "source": [
        "p_ygivenx_logistic_regression.sample() # sampling the predicted labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([8, 0, 2, 4, 2, 8, 3, 0, 7, 0], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCkZytXzwQDl",
        "outputId": "7362d49a-df06-4103-da69-9ceb8b5a79be"
      },
      "source": [
        "p_ygivenx_logistic_regression.mode() # looking at the most probable labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 0, 3, 8, 8, 8, 3, 0, 8, 0], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD_RnKqm1W1N"
      },
      "source": [
        "# Training the logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3E0_KzB1dRE"
      },
      "source": [
        "To train the classifier, we define a function that performs a gradient descent step. First, we choose the flavour of SGD that we want (in this case, the [fairly famous Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgMTlL-2wQFf"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5wus2n2Tubq"
      },
      "source": [
        "@tf.function\n",
        "def train_step_logistic_regression(data, labels):\n",
        "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\n",
        "    p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(data)) # One could also use logits rather than probs and remove the softmax layer...\n",
        "    logp_ygivenx_logistic_regression = p_ygivenx_logistic_regression.log_prob(labels)\n",
        "    loss = -tf.reduce_mean(logp_ygivenx_logistic_regression)  # the loss is the average negative log likelihood\n",
        "  gradients = tape.gradient(loss, logistic_regression.trainable_variables)  # here, the gradient is automatically computed\n",
        "  optimizer.apply_gradients(zip(gradients, logistic_regression.trainable_variables))  # Adam iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EmfozUiTueC"
      },
      "source": [
        "@tf.function\n",
        "def evaluate_logistic_regression(data, labels):\n",
        "  p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(data))\n",
        "  logp_ygivenx_logistic_regression = p_ygivenx_logistic_regression.log_prob(labels)\n",
        "  log_likelihood = tf.reduce_mean(logp_ygivenx_logistic_regression)\n",
        "  y_pred = p_ygivenx_logistic_regression.mode()\n",
        "  acc = tf.reduce_mean(tf.cast(y_pred == labels,tf.float32))\n",
        "  return acc, log_likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sRf2jgK4W9s",
        "outputId": "b5545083-c02d-451f-bbd5-0913a2ddedcc"
      },
      "source": [
        " evaluate_logistic_regression(train_images,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=0.09393334>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=-13.758713>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfcpUtie4_MC"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,y_train)).shuffle(60000).batch(32) # TF creates the batches for us"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_XF8G4kTugJ",
        "outputId": "6329f2d8-16c6-4e24-a642-72eb9ab8cae8"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for images, labels in train_dataset:\n",
        "    train_step_logistic_regression(images, labels) # Adam iteration\n",
        "  acc, log_likelihood = evaluate_logistic_regression(train_images,y_train)\n",
        "  acc_test, log_likelihood_test = evaluate_logistic_regression(test_images,y_test)\n",
        "  print('Epoch  %g' %epoch)\n",
        "  print('Train accuracy  %g' %acc.numpy())\n",
        "  print('Test accuracy  %g' %acc_test.numpy())\n",
        "  print('Train log-likelihood  %g' %log_likelihood.numpy())\n",
        "  print('Test log-likelihood  %g' %log_likelihood_test.numpy())\n",
        "  print('-----------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1\n",
            "Train accuracy  0.1246\n",
            "Test accuracy  0.1305\n",
            "Train log-likelihood  -9.50821\n",
            "Test log-likelihood  -9.32266\n",
            "-----------\n",
            "Epoch  2\n",
            "Train accuracy  0.189433\n",
            "Test accuracy  0.2023\n",
            "Train log-likelihood  -7.02632\n",
            "Test log-likelihood  -6.82459\n",
            "-----------\n",
            "Epoch  3\n",
            "Train accuracy  0.284317\n",
            "Test accuracy  0.2963\n",
            "Train log-likelihood  -5.36089\n",
            "Test log-likelihood  -5.16961\n",
            "-----------\n",
            "Epoch  4\n",
            "Train accuracy  0.380383\n",
            "Test accuracy  0.3898\n",
            "Train log-likelihood  -4.25872\n",
            "Test log-likelihood  -4.0787\n",
            "-----------\n",
            "Epoch  5\n",
            "Train accuracy  0.455667\n",
            "Test accuracy  0.4668\n",
            "Train log-likelihood  -3.50228\n",
            "Test log-likelihood  -3.33061\n",
            "-----------\n",
            "Epoch  6\n",
            "Train accuracy  0.517217\n",
            "Test accuracy  0.5274\n",
            "Train log-likelihood  -2.96103\n",
            "Test log-likelihood  -2.80156\n",
            "-----------\n",
            "Epoch  7\n",
            "Train accuracy  0.565583\n",
            "Test accuracy  0.5775\n",
            "Train log-likelihood  -2.56338\n",
            "Test log-likelihood  -2.4147\n",
            "-----------\n",
            "Epoch  8\n",
            "Train accuracy  0.605117\n",
            "Test accuracy  0.6177\n",
            "Train log-likelihood  -2.26202\n",
            "Test log-likelihood  -2.12532\n",
            "-----------\n",
            "Epoch  9\n",
            "Train accuracy  0.637067\n",
            "Test accuracy  0.6525\n",
            "Train log-likelihood  -2.02775\n",
            "Test log-likelihood  -1.90151\n",
            "-----------\n",
            "Epoch  10\n",
            "Train accuracy  0.664017\n",
            "Test accuracy  0.6769\n",
            "Train log-likelihood  -1.84343\n",
            "Test log-likelihood  -1.72706\n",
            "-----------\n",
            "Epoch  11\n",
            "Train accuracy  0.686133\n",
            "Test accuracy  0.7001\n",
            "Train log-likelihood  -1.69439\n",
            "Test log-likelihood  -1.5864\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWZZZwYgz3BK"
      },
      "source": [
        "**Question 2.** Compare the results of your logistic regression classifier with the ones given by scikit-learn's logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Ju1xtK0Cr8"
      },
      "source": [
        "**Question 3.** Replace the logistic regression model by a deep classifier of your choice (e.g. a MLP or a CNN). Try to beat logistic regression!"
      ]
    }
  ]
}