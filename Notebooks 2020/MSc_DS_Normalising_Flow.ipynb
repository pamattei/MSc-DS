{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSc_DS_Normalising_Flow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrfG6ksJMWpq"
      },
      "source": [
        "We'll implement a Real NVP on Iris."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWGmwtSm-O3A"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import scipy.stats\r\n",
        "import scipy.io\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "import scipy.sparse\r\n",
        "from scipy.io import loadmat\r\n",
        "import pandas as pd\r\n",
        "import tensorflow_probability as tfp\r\n",
        "tfd = tfp.distributions\r\n",
        "tfb = tfp.bijectors\r\n",
        "tfk = tf.keras\r\n",
        "tfkl = tf.keras.layers\r\n",
        "from PIL import Image\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhOhNRO7e8BD",
        "outputId": "3db34740-62fc-4696-fd75-da43d78bf996"
      },
      "source": [
        "!git clone https://github.com/LukasRinder/normalizing-flows.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'normalizing-flows' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7cF5oRffcXX",
        "outputId": "84bb9f44-e2a4-4b65-faef-c2620610f4d2"
      },
      "source": [
        "%cd normalizing-flows/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/normalizing-flows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IyrjE4-fzlV"
      },
      "source": [
        "import normalizingflows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjfcBkbWfX42",
        "outputId": "943f1ace-7c33-4bdb-9243-7161a02060d0"
      },
      "source": [
        "from normalizingflows.flow_catalog import RealNVP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow:  2.4.0\n",
            "tensorflow-probability:  0.12.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJMpADCQ-zdw"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "data = load_iris(True)[0]\r\n",
        "\r\n",
        "x = ((data - np.mean(data,0))/np.std(data,0)).astype(np.float32)\r\n",
        "n = x.shape[0] # number of observations\r\n",
        "p = x.shape[1] # number of feat*ures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3wXZVRff8Hn"
      },
      "source": [
        "layers = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQX7K3U9cJID"
      },
      "source": [
        "permutation = tf.cast(np.concatenate((np.arange(p/2,p),np.arange(0,p/2))), tf.int32)\r\n",
        "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(p, tf.float32))\r\n",
        "\r\n",
        "bijectors = []\r\n",
        "\r\n",
        "for i in range(layers):\r\n",
        "    bijectors.append(tfb.BatchNormalization())\r\n",
        "    bijectors.append(RealNVP(input_shape=p, n_hidden=[32,32]))\r\n",
        "    bijectors.append(tfp.bijectors.Permute(permutation))\r\n",
        "\r\n",
        "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_real_nvp')\r\n",
        "\r\n",
        "flow = tfd.TransformedDistribution(\r\n",
        "    distribution=base_dist,\r\n",
        "    bijector=bijector\r\n",
        ")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6_OIJ8pjC7t"
      },
      "source": [
        "params =  flow.trainable_variables\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARbc2eCwksye"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(data):\r\n",
        "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\r\n",
        "    loss = -tf.reduce_mean(flow.log_prob(data))  # the loss is the average negative log likelihood\r\n",
        "  gradients = tape.gradient(loss, params)  # here, the gradient is automatically computed\r\n",
        "  optimizer.apply_gradients(zip(gradients, params))  # Adam iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnW6B_vWks0t"
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices(x).shuffle(n).batch(32) # Batches of size 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txBR2pW7kscz",
        "outputId": "d91745b8-9f3c-48d7-da89-26ea8521150d"
      },
      "source": [
        "EPOCHS = 20001\r\n",
        "\r\n",
        "for epoch in range(1,EPOCHS+1):\r\n",
        "  for data in train_data:\r\n",
        "    train_step(data) # Adam iteration\r\n",
        "  if (epoch % 1000) == 1:\r\n",
        "    ll_train = tf.reduce_mean(flow.log_prob(data))\r\n",
        "    print('Epoch  %g' %epoch)\r\n",
        "    print('Training log-likelihood %g' %ll_train.numpy())\r\n",
        "    print('-----------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2273: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  1\n",
            "Training log-likelihood -6.2762\n",
            "-----------\n",
            "Epoch  1001\n",
            "Training log-likelihood -2.96461\n",
            "-----------\n",
            "Epoch  2001\n",
            "Training log-likelihood -2.24732\n",
            "-----------\n",
            "Epoch  3001\n",
            "Training log-likelihood -2.2694\n",
            "-----------\n",
            "Epoch  4001\n",
            "Training log-likelihood -2.41465\n",
            "-----------\n",
            "Epoch  5001\n",
            "Training log-likelihood -1.64345\n",
            "-----------\n",
            "Epoch  6001\n",
            "Training log-likelihood -2.07701\n",
            "-----------\n",
            "Epoch  7001\n",
            "Training log-likelihood -2.51249\n",
            "-----------\n",
            "Epoch  8001\n",
            "Training log-likelihood -2.58125\n",
            "-----------\n",
            "Epoch  9001\n",
            "Training log-likelihood -1.72651\n",
            "-----------\n",
            "Epoch  10001\n",
            "Training log-likelihood -1.42071\n",
            "-----------\n",
            "Epoch  11001\n",
            "Training log-likelihood -1.47177\n",
            "-----------\n",
            "Epoch  12001\n",
            "Training log-likelihood -1.69175\n",
            "-----------\n",
            "Epoch  13001\n",
            "Training log-likelihood -2.11553\n",
            "-----------\n",
            "Epoch  14001\n",
            "Training log-likelihood -2.20688\n",
            "-----------\n",
            "Epoch  15001\n",
            "Training log-likelihood -1.653\n",
            "-----------\n",
            "Epoch  16001\n",
            "Training log-likelihood -2.02274\n",
            "-----------\n",
            "Epoch  17001\n",
            "Training log-likelihood -1.7514\n",
            "-----------\n",
            "Epoch  18001\n",
            "Training log-likelihood -1.48546\n",
            "-----------\n",
            "Epoch  19001\n",
            "Training log-likelihood -1.89442\n",
            "-----------\n",
            "Epoch  20001\n",
            "Training log-likelihood -1.85247\n",
            "-----------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}